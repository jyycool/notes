# 01 | Flink 集群构建 & 逻辑计划生成

## 一、准备工作

github 上拉取最新的源码到本地, 然后将源码导入 IDEA 编译。

> Flink-1.11 在编译过程中可能会遇到如下报错:
>
> java: 警告: 源发行版 11 需要目标发行版 11
>
> 需要使用 Java 11 编译, 将 java_home 指向 JDK11, 然后重新编译

在真正执行 Flink 作业之前, 有几个问题需要解决:

1. Flink 作业是如何提交到集群的?
2. Flink 集群是如何在资源管理集群上启动起来的?
3. Flink 的计算资源是如何分配给作业的?
4. Flink 作业提交后如何启动的?

## 二、提交命令分析

flink 作业的提交命令如下:

```sh
${FLINK_HOME}/bin/flink run \
-m yarn-cluster \
-yjm 2048 \
-ytm 2048 \
-ys 2 \
-p 2 \
/Users/sherlock/devTools/opt/flink-1.11.3/examples/streaming/TopSpeedWindowing.jar
```

bin/flink 脚本内容如下

```sh
# 获取当前脚本名
target="$0"
iteration=0
# 判断脚本名是否符合规则
while [ -L "$target" ]; do
    if [ "$iteration" -gt 100 ]; then
        echo "Cannot resolve path: You have a cyclic symlink in $target."
        break
    fi
    ls=`ls -ld -- "$target"`
    target=`expr "$ls" : '.* -> \(.*\)$'`
    iteration=$((iteration + 1))
done

# 将相对路径转换为绝对路径
bin=`dirname "$target"`

# 调用config.sh脚本，获取flink配置
. "$bin"/config.sh
# 给FLINK_IDENT_STRING变量赋值，如果为空则为当前用户
if [ "$FLINK_IDENT_STRING" = "" ]; then
        FLINK_IDENT_STRING="$USER"
fi

CC_CLASSPATH=`constructFlinkClassPath`

# 设置日志输出
log=$FLINK_LOG_DIR/flink-$FLINK_IDENT_STRING-client-$HOSTNAME.log
log_setting=(-Dlog.file="$log" -Dlog4j.configuration=file:"$FLINK_CONF_DIR"/log4j-cli.properties -Dlogback.configurationFile=file:"$FLINK_CONF_DIR"/logback.xml)

# 添加环境变量
export FLINK_ROOT_DIR
export FLINK_CONF_DIR

# 添加HADOOP_CLASSPATH 以允许使用HDFS  
# 使用exec启用新进程，调用java方法org.apache.flink.client.cli.CliFrontend 
exec $JAVA_RUN $JVM_ARGS "${log_setting[@]}" -classpath "`manglePathList "$CC_CLASSPATH:$INTERNAL_HADOOP_CLASSPATHS"`" org.apache.flink.client.cli.CliFrontend "$@"
```

这个脚本不长，其中做了2个主要的操作

1. 调用config.sh脚本，这个脚本的主要作用是加载 FLINK 的 ClassPath，加载 Flink 配置

2. 调用exec，执行 java 类 org.apache.flink.client.cli.CliFrontend 作为程序的入口，并将命令行的输入作为参数。

这是执行 flink run 脚本后，最后真正执行的命令实例：

```sh
exec /opt/jdk1.8.0_211/bin/java \
-Dlog.file=/opt/flink-1.8.0/log/flink-root-client-louisvv.log \
-Dlog4j.configuration=file:/opt/flink-1.8.0/conf/log4j-cli.properties \
-Dlogback.configurationFile=file:/opt/flink-1.8.0/conf/logback.xml \
-classpath /opt/flink-1.8.0/lib/log4j-1.2.17.jar:/opt/flink-1.8.0/lib/slf4j-log4j12-1.7.15.jar:/opt/flink-1.8.0/lib/flink-dist_2.12-1.8.0.jar org.apache.flink.client.cli.CliFrontend run -m yarn-cluster -yjm 2048 -ytm 2048 -ys 2 -p 2 /Users/sherlock/devTools/opt/flink-1.11.3/examples/streaming/TopSpeedWindowing.jar
```

重点：

1. 加载 Flink classpath

   -classpath /opt/flink-1.8.0/lib/log4j-1.2.17.jar:/opt/flink-1.8.0/lib/slf4j-log4j12-1.7.15.jar:/opt/flink-1.8.0/lib/flink-dist_2.12-1.8.0.jar

2. 调用程序主类 org.apache.flink.client.cli.CliFrontend 类，参数为用户提交 job 的自定义参数: `run -m yarn-cluster -yjm 2048 -ytm 2048 -ys 2 -p 2 ~/devTools/opt/flink-1.8.0/examples/streaming/TopSpeedWindowing.jar`

## 三、提交流程分析 

现在, 我们知道了提交任务的实质就是执行了 `CliFrontend run` , 进入 IDEA, 开始 Debug 之旅.

首先找到类 `org.apache.flink.client.cli.CliFrontend.main` 添加 args: `run -m yarn-cluster -yjm 2048 -ytm 2048 -ys 2 -p 2 ~/devTools/opt/flink-1.8.0/examples/streaming/TopSpeedWindowing.jar`

### 3.1 入口 CliFrontend # main

`CliFrontend.main`

```java
public static void main(final String[] args) {
		EnvironmentInformation.logEnvironmentInfo(
      LOG, "Command Line Client", args);

		// 1. find the configuration directory
		final String configurationDirectory = getConfigurationDirectoryFromEnv();

		// 2. load the global configuration
		final Configuration configuration = GlobalConfiguration.loadConfiguration(configurationDirectory);

		// 3. load the custom command lines
		final List<CustomCommandLine> customCommandLines = loadCustomCommandLines(
			configuration,
			configurationDirectory);

		try {
			final CliFrontend cli = new CliFrontend(
				configuration,
				customCommandLines);

			SecurityUtils.install(new SecurityConfiguration(cli.configuration));
			int retCode = SecurityUtils.getInstalledContext()
					.runSecured(() -> cli.parseParameters(args));
			System.exit(retCode);
		}
		catch (Throwable t) {
			final Throwable strippedThrowable = ExceptionUtils.stripException(t, UndeclaredThrowableException.class);
			LOG.error("Fatal error while running command line interface.", strippedThrowable);
			strippedThrowable.printStackTrace();
			System.exit(31);
		}
	}
```

这个函数中主要做了以下几件事:

1. 调用方法 `CliFrontend # getConfigurationDirectoryFromEnv()` 获取配置文件目录

2. 调用方法 `GlobalConfiguration # loadConfiguration(String)` 加载全局配置

3. 调用方法 `CliFrontend # loadCustomCommandLines(Configuration, String)` 加载用户输入命令行参数

4. 调用构造方法 `CliFrontend(Configutation, List<CustomCommandLine>)` 实例化 CliFrontend，构造方法中的逻辑:
   1. 检查配置、命令行是否为空
   2. 初始化文件系统配置
   3. 解析命令行命令相关配置参数 ，并添加到执行配置中
   4. 设置客户端超时时间
   5. 设置默认的并行度

5. 调用方法 `SecurityUtils # install(SecurityConfiguration)` 加载安全配置模块

6. 匹配用户命令行参数 \<ACTION> 并执行对应的 \<ACTION> 方法(在异步线程中执行)，之后立即返回 0/1

7. 主线程获取执行返回值，关闭客户端

#### 3.1.1 查找配置文件目录

`CliFrontend # getConfigurationDirectoryFromEnv()`

```java
public static String getConfigurationDirectoryFromEnv() {
  
  // 从环境中找到 FLINK_CONF_DIR 的值, 存在则返回, 否则报错
  String location = System.getenv(ConfigConstants.ENV_FLINK_CONF_DIR);

  if (location != null) {
    if (new File(location).exists()) {
      return location;
    }
    else {
      throw new RuntimeException("The configuration directory '" + location + "', specified in the '" +
                                 ConfigConstants.ENV_FLINK_CONF_DIR + "' environment variable, does not exist.");
    }
  }
  else if (new File(CONFIG_DIRECTORY_FALLBACK_1).exists()) {
    location = CONFIG_DIRECTORY_FALLBACK_1;
  }
  else if (new File(CONFIG_DIRECTORY_FALLBACK_2).exists()) {
    location = CONFIG_DIRECTORY_FALLBACK_2;
  }
  else {
    throw new RuntimeException("The configuration directory was not specified. " +
                               "Please specify the directory containing the configuration file through the '" +
                               ConfigConstants.ENV_FLINK_CONF_DIR + "' environment variable.");
  }
  return location;
}
```

这个方法逻辑超简单: 直接从环境变量中找到 FLINK_CONF_DIR 的值, 存在则返回, 否则报错。

#### 3.1.2 加载全局配置

`GlobalConfiguration # loadConfiguration(String);`

```java
public static Configuration loadConfiguration(
  final String configDir, 
  @Nullable final Configuration dynamicProperties) {

	// configDir 不可为 null
  if (configDir == null) {
    throw new IllegalArgumentException("Given configuration directory is null, cannot load configuration");
  }

  // 配置目录必须存在
  final File confDirFile = new File(configDir);
  if (!(confDirFile.exists())) {
    throw new IllegalConfigurationException(
      "The given configuration directory name '" + configDir +
      "' (" + confDirFile.getAbsolutePath() + ") does not describe an existing directory.");
  }

  // 配置目录下 flink 配置文件 flink-conf.yaml 必须存在
  final File yamlConfigFile = new File(confDirFile, FLINK_CONF_FILENAME);

  if (!yamlConfigFile.exists()) {
    throw new IllegalConfigurationException(
      "The Flink config file '" + yamlConfigFile +
      "' (" + confDirFile.getAbsolutePath() + ") does not exist.");
  }

  Configuration configuration = loadYAMLResource(yamlConfigFile);

  if (dynamicProperties != null) {
    configuration.addAll(dynamicProperties);
  }

  return configuration;
}
```

方法逻辑:

1. 检查配置目录是否存在
2. 配置目录下配置文件 FLINK_CONF_FILENAME(flink-conf.yaml) 是否存在

#### 3.1.3 加载用户命令行参数

`CliFrontend # loadCustomCommandLines(Configuration, String)`

```java
public static List<CustomCommandLine> loadCustomCommandLines(
  	Configuration configuration, 
  	String configurationDirectory) {
  
  List<CustomCommandLine> customCommandLines = new ArrayList<>();
  // GenericCLI 是 CustomCommandLine 的一个通用实现，将显式指定的目标参数 -D 转发给相应的 PipelineExecutor，以便进一步解析。
  customCommandLines.add(new GenericCLI(
    configuration, 
    configurationDirectory));

  //	Command line interface of the YARN session, with a special initialization here
  //	to prefix all options with y/yarn.
  final String flinkYarnSessionCLI = "org.apache.flink.yarn.cli.FlinkYarnSessionCli";
  try {
    customCommandLines.add(
      loadCustomCommandLine(flinkYarnSessionCLI,
                            configuration,
                            configurationDirectory,
                            "y",
                            "yarn"));
  } catch (NoClassDefFoundError | Exception e) {
    final String errorYarnSessionCLI = "org.apache.flink.yarn.cli.FallbackYarnSessionCli";
    try {
      LOG.info("Loading FallbackYarnSessionCli");
      customCommandLines.add(
        loadCustomCommandLine(errorYarnSessionCLI, configuration));
    } catch (Exception exception) {
      LOG.warn("Could not load CLI class {}.", flinkYarnSessionCLI, e);
    }
  }

  //	Tips: DefaultCLI must be added at last, because getActiveCustomCommandLine(..) will get the
  //	      active CustomCommandLine in order and DefaultCLI isActive always return true.
  customCommandLines.add(new DefaultCLI(configuration));

  return customCommandLines;
}
```

这个方法乍一看完全云里雾里, 因为我们知道 Flink 的部署有好几种模式, StandAlone, Yarn-session, yarn-perjob, 所以这里 GenericCLI, FlinkYarnSessionCli, DefaultCLI, 分别对应上面的部署方式。具体可以到各个类中查看。

在 debug 的时候发现个有意思的事情 customCommandLines 这个 List 在方法调用完后里面只有两个元素分别是 GenericCLI, DefaultCLI。

**customCommandLines:List[CustomCommandLine] 中为什么没有 FlinkYarnSessionCli ?** 

进入源码方法 `CliFrontend # loadCustomCommandLine(String, Object...)` 

```java
private static CustomCommandLine loadCustomCommandLine(String className, Object... params) throws Exception {

		Class<? extends CustomCommandLine> customCliClass =
			Class.forName(className).asSubclass(CustomCommandLine.class);

		// construct class types from the parameters
		Class<?>[] types = new Class<?>[params.length];
		for (int i = 0; i < params.length; i++) {
			checkNotNull(params[i], "Parameters for custom command-lines may not be null.");
			types[i] = params[i].getClass();
		}

		Constructor<? extends CustomCommandLine> constructor = customCliClass.getConstructor(types);

		return constructor.newInstance(params);
	}

}
```

`Class # forName(className)` 该行代码会抛出如下报错: 

> java.lang.ClassNotFoundException:org.apache.flink.yarn.cli.FlinkYarnSessionCli

跟进方法 `Class # forName(className)`  -> `forName0(className, true, ClassLoader.getClassLoader(caller), caller)` 到这里因为 forName0 是 native 方法, 所以要一步步调试该方法里的参数 `ClassLoader.getClassLoader(caller)`。最后就会发现, ClassLoader 确实加载不到 `FlinkYarnSessionCli`。

于是换了个思路, 如果这个类所在包如果没有被加入依赖呢? 查看当前模块 flink-clients 的 pom.xml 文件中的 dependencies, 确实没有引入对模块 flink-yarn(FlinkYarnSessionCli类所在模块)的依赖, 又想到 session 模式, 需要执行脚本 yarn-session.sh

```sh
bin=`dirname "$0"`
bin=`cd "$bin"; pwd`

# get Flink config
. "$bin"/config.sh

if [ "$FLINK_IDENT_STRING" = "" ]; then
        FLINK_IDENT_STRING="$USER"
fi

JVM_ARGS="$JVM_ARGS -Xmx512m"

CC_CLASSPATH=`manglePathList $(constructFlinkClassPath):$INTERNAL_HADOOP_CLASSPATHS`

log=$FLINK_LOG_DIR/flink-$FLINK_IDENT_STRING-yarn-session-$HOSTNAME.log
log_setting="-Dlog.file="$log" -Dlog4j.configuration=file:"$FLINK_CONF_DIR"/log4j-session.properties -Dlog4j.configurationFile=file:"$FLINK_CONF_DIR"/log4j-session.properties -Dlogback.configurationFile=file:"$FLINK_CONF_DIR"/logback-session.xml"

$JAVA_RUN $JVM_ARGS -classpath "$CC_CLASSPATH" $log_setting org.apache.flink.yarn.cli.FlinkYarnSessionCli -j "$FLINK_LIB_DIR"/flink-dist*.jar "$@"
```

最后一行, 执行了"org.apache.flink.yarn.cli.FlinkYarnSessionCli" 中的 main。

#### 3.1.4 创建CliFrontend对象

`CliFrontend(Configutation, ClusterClientServiceLoader, List<CustomCommandLine>)`

```java
public CliFrontend(
  Configuration configuration,
  ClusterClientServiceLoader clusterClientServiceLoader,
  List<CustomCommandLine> customCommandLines) {
  
  this.configuration = checkNotNull(configuration);
  this.customCommandLines = checkNotNull(customCommandLines);
  this.clusterClientServiceLoader = checkNotNull(clusterClientServiceLoader);

  FileSystem.initialize(configuration, PluginUtils.createPluginManagerFromRootFolder(configuration));

  this.customCommandLineOptions = new Options();

  for (CustomCommandLine customCommandLine : customCommandLines) {
    customCommandLine.addGeneralOptions(customCommandLineOptions);
    customCommandLine.addRunOptions(customCommandLineOptions);
  }

  this.clientTimeout = configuration.get(ClientOptions.CLIENT_TIMEOUT);
  this.defaultParallelism = configuration.getInteger(CoreOptions.DEFAULT_PARALLELISM);
}
```

1. 这里调用的 checkNotNull() 方法是检查 configuration 和 customCommandLines 是否为空

2. 检查过后，调用 FileSystem 的 initialize 初始化方法，将文件系统初始化

   Flink 通过 FileSystem 类来抽象自己的文件系统，这个抽象提供了各类文件系统实现的通用操作和最低保证。

   其中，initialize 为静态方法，用来初始化共享文件系统设置

   在 Flink 的配置文件中，也需要我们去指定 Filesystem 的配置，一般来说使用的是HDFS

   在初始化方法 initialize 中

   （1）遍历传递的configuration ，获取Filesystem的配置，并通过工厂加载

   （2）如果获取到配置，则加载默认的文件系统

   （3）获取默认文件系统的scheme

3. 获取用户命令行配置 customCommandLines，遍历list将其添加到运行配置和一般配置中

4. 获取客户端超时时间，默认60000毫秒

5. 获取默认并行度，默认并行度为1

Flink 的 FileSystem.initialize() 代码如下：

```java
public static void initialize(
  Configuration config,
  PluginManager pluginManager) throws IllegalConfigurationException {

  LOCK.lock();
  try {
    // make sure file systems are re-instantiated after re-configuration
    CACHE.clear();
    FS_FACTORIES.clear();

    Collection<Supplier<Iterator<FileSystemFactory>>> factorySuppliers = new ArrayList<>(2);
    factorySuppliers.add(() -> ServiceLoader.load(FileSystemFactory.class).iterator());

    if (pluginManager != null) {
      factorySuppliers.add(() ->
                           Iterators.transform(pluginManager.load(FileSystemFactory.class), PluginFileSystemFactory::of));
    }

    final List<FileSystemFactory> fileSystemFactories = loadFileSystemFactories(factorySuppliers);

    // configure all file system factories
    for (FileSystemFactory factory : fileSystemFactories) {
      factory.configure(config);
      String scheme = factory.getScheme();

      FileSystemFactory fsf = ConnectionLimitingFactory.decorateIfLimited(factory, scheme, config);
      FS_FACTORIES.put(scheme, fsf);
    }

    // configure the default (fallback) factory
    FALLBACK_FACTORY.configure(config);

    // also read the default file system scheme
    final String stringifiedUri = config.getString(CoreOptions.DEFAULT_FILESYSTEM_SCHEME, null);
    if (stringifiedUri == null) {
      defaultScheme = null;
    }
    else {
      try {
        defaultScheme = new URI(stringifiedUri);
      }
      catch (URISyntaxException e) {
        throw new IllegalConfigurationException("The default file system scheme ('" +
                                                CoreOptions.DEFAULT_FILESYSTEM_SCHEME + "') is invalid: " + stringifiedUri, e);
      }
    }

    ALLOWED_FALLBACK_FILESYSTEMS.clear();
    final Iterable<String> allowedFallbackFilesystems = Splitter.on(';').omitEmptyStrings().trimResults()
      .split(config.getString(CoreOptions.ALLOWED_FALLBACK_FILESYSTEMS));
    allowedFallbackFilesystems.forEach(ALLOWED_FALLBACK_FILESYSTEMS::add);
  }
  finally {
    LOCK.unlock();
  }
}
```

#### 3.1.5 加载安全配置模块

`org.apache.flink.runtime.security.SecurityConfiguration` 类，是flink全局安全配置，这里的代码暂时略过。

#### 3.1.6 匹配Action后执行

调用 CliFrontend 的 parseParameters() 方法，解析命令行参数，进行模式匹配。这个步骤是在异步线程中执行的。

```java
public int parseParameters(String[] args) {

  // check for action
  if (args.length < 1) {
    CliFrontendParser.printHelp(customCommandLines);
    System.out.println("Please specify an action.");
    return 1;
  }

  // get action
  String action = args[0];

  // remove action from parameters
  final String[] params = Arrays.copyOfRange(args, 1, args.length);

  try {
    // do action
    switch (action) {
      case ACTION_RUN:
        run(params);
        return 0;
      case 
        ......
      default:
      	......
    }
  } catch (CliArgsException ce) {
    return handleArgException(ce);
  } catch (ProgramParametrizationException ppe) {
    return handleParametrizationException(ppe);
  } catch (ProgramMissingJobException pmje) {
    return handleMissingJobException();
  } catch (Exception e) {
    return handleError(e);
  }
}
```

提交 job 对应的 Action 就是 run。所以接着异步执行 run 方法。并立即给主线程返回匹配结果: 正常返回 0，匹配异常返回 1。

#### 3.1.7 关闭 Client

主线程 main 根据 3.1.6 的执行结果获取返回值，关闭client

## 四、JobGraph的提交

接上 3.1.6 匹配 Action 的分析, 根据“action”进行模式匹配，如果是run，则执行对应的逻辑

```java
case ACTION_RUN:
    run(params);
    return 0;
```

`CliFrontend # run(String[])` 执行逻辑：

1. 获取配置信息
2. 解析命令行参数，如果参数不符合规则，则打印 Flink 的使用帮助
3. 检查用户指定jar包路径是否为空，如果为空，则打印异常信息，并抛出异常 CliArgsException，前面的模式匹配接收到异常后，对异常进行处理，打印出帮助信息
4. 从用户的提交jar中解析出 PackagedProgram
5. 获取 PackagedProgram 中 job 依赖
6. 反射调用用户 jar 中的 main

```java
protected void run(String[] args) throws Exception {
  LOG.info("Running 'run' command.");

  final Options commandOptions = CliFrontendParser.getRunCommandOptions();
  
  final CommandLine commandLine = getCommandLine(commandOptions, args, true);

  // 打印 help 信息
  if (commandLine.hasOption(HELP_OPTION.getOpt())) {
    CliFrontendParser.printHelpForRun(customCommandLines);
    return;
  }
	
	// 从[FlinkYarnSessionCli, DeafultCli] 中获取激活的 CommandLine
  final CustomCommandLine activeCommandLine =
    validateAndGetActiveCommandLine(checkNotNull(commandLine));
	// 解析作业配置参数
  final ProgramOptions programOptions = ProgramOptions.create(commandLine);
	// 从用户的提交jar中解析出 PackagedProgram
  final PackagedProgram program =
    getPackagedProgram(programOptions);
	// 获取 PackagedProgram 中 job 依赖
  final List<URL> jobJars = program.getJobJarAndDependencies();
  final Configuration effectiveConfiguration = getEffectiveConfiguration(
    activeCommandLine, commandLine, programOptions, jobJars);

  LOG.debug("Effective executor configuration: {}", effectiveConfiguration);

  try {
    // 这里开始反射调用用户 jar 中的 main
    executeProgram(effectiveConfiguration, program);
  } finally {
    program.deleteExtractedLibraries();
  }
}

private PackagedProgram getPackagedProgram(ProgramOptions programOptions) throws ProgramInvocationException, CliArgsException {
  PackagedProgram program;
  try {
    LOG.info("Building program from JAR file");
    program = buildProgram(programOptions);
  } catch (FileNotFoundException e) {
    throw new CliArgsException("Could not build the program from JAR file: " + e.getMessage(), e);
  }
  return program;
}
```

### 4.1 StreamGraph

从这开始的逻辑就要从用户作业最后的 `StreamExecutionEnvironment # execute(String)` 开始看了。

```java
// StreamExecutionEnvironment # execute(String)
public JobExecutionResult execute(String jobName) throws Exception {
  Preconditions.checkNotNull(jobName, "Streaming Job name should not be null.");

  return execute(getStreamGraph(jobName));
}
```

获取 StreamGraph 的逻辑就从这里开始了...一步步跟进去

```java
// StreamExecutionEnvironment # getStreamGraph(String, boolean)
public StreamGraph getStreamGraph(String jobName, boolean clearTransformations) {
  // 这里是真正获取 StreamGraph 的逻辑方法
  StreamGraph streamGraph = getStreamGraphGenerator().setJobName(jobName).generate();
  if (clearTransformations) {
    this.transformations.clear();
  }
  return streamGraph;
}

// StreamExecutionEnvironment # getStreamGraphGenerator()
private StreamGraphGenerator getStreamGraphGenerator() {
  // 用户代码中调用 map, filter, ..., 这些 operator 都会被封装成一个个 Transformation 装载到 StreamExecutionEnvironment 的 transformations 这个 List 中
  if (transformations.size() <= 0) {
    throw new IllegalStateException("No operators defined in streaming topology. Cannot execute.");
  }
  // 这里做生成 Graph 前的一些设置工作, 例如 operator chain, stateBackend,.....
  return new StreamGraphGenerator(transformations, config, checkpointCfg)
    .setStateBackend(defaultStateBackend)
    .setChaining(isChainingEnabled)
    .setUserArtifacts(cacheFile)
    .setTimeCharacteristic(timeCharacteristic)
    .setDefaultBufferTimeout(bufferTimeout);
}

// StreamGraphGenerator # generate()
public StreamGraph generate() {
  // 将 Graph 的设置信息 set 进去
  streamGraph = new StreamGraph(executionConfig, checkpointConfig, savepointRestoreSettings);
  streamGraph.setStateBackend(stateBackend);
  streamGraph.setChaining(chaining);
  streamGraph.setScheduleMode(scheduleMode);
  streamGraph.setUserArtifacts(userArtifacts);
  streamGraph.setTimeCharacteristic(timeCharacteristic);
  streamGraph.setJobName(jobName);
  streamGraph.setGlobalDataExchangeMode(globalDataExchangeMode);
	
  alreadyTransformed = new HashMap<>();
	
  // 对集合中的每个transformation进行转换。
  for (Transformation<?> transformation: transformations) {
    transform(transformation);
  }

  final StreamGraph builtStreamGraph = streamGraph;

  alreadyTransformed.clear();
  alreadyTransformed = null;
  streamGraph = null;

  return builtStreamGraph;
}
```

这里贴一下这个 transformations 集合

测试代码的逻辑是: `readTextFile() -> filter -> map -> assignTimestampsAndWatermarks-> keyBy -> process -> print`, 它生成的 transformations:List[6] 是这样的:

```apl
List[0]=> OneInputTransformation{id=2, name='Split Reader: Custom File Source', outputType=String, parallelism=1}

List[1]=> OneInputTransformation{id=3, name='Filter', outputType=String, parallelism=1}

List[2]=> OneInputTransformation{id=4, name='Map', outputType=GenericType<cgs.flink.utils.MockViewer>, parallelism=1}

List[3]=> OneInputTransformation{id=5, name='Timestamps/Watermarks', outputType=GenericType<cgs.flink.utils.MockViewer>, parallelism=1}

List[4]=> OneInputTransformation{id=7, name='KeyedProcess', outputType=GenericType<cgs.flink.window.PvAndLastDate>, parallelism=1}

List[5]=> SinkTransformation{id=8, name='Print to Std. Out', outputType=GenericType<java.lang.Object>, parallelism=1}
```

`StreamGraphGenerator # transformation()`

```java
// 对具体的一个transformation进行转换，转换成 StreamGraph 中的 StreamNode 和 StreamEdge
// 返回值为该transform的id集合，通常大小为1个（除FeedbackTransformation）
private Collection<Integer> transform(Transformation<?> transform) {

  // 跳过已经转换过的transformation
  if (alreadyTransformed.containsKey(transform)) {
    return alreadyTransformed.get(transform);
  }

  LOG.debug("Transforming " + transform);

  if (transform.getMaxParallelism() <= 0) {

    // if the max parallelism hasn't been set, then first use the job wide max parallelism
    // from the ExecutionConfig.
    int globalMaxParallelismFromConfig = executionConfig.getMaxParallelism();
    if (globalMaxParallelismFromConfig > 0) {
      transform.setMaxParallelism(globalMaxParallelismFromConfig);
    }
  }

  // 为了触发 MissingTypeInfo 的异常
  transform.getOutputType();

  // 调用 transformXXX 来对具体的StreamTransformation进行转换。
  Collection<Integer> transformedIds;
  if (transform instanceof OneInputTransformation<?, ?>) {
    transformedIds = transformOneInputTransform(
      (OneInputTransformation<?, ?>) transform);
  } else if (transform instanceof TwoInputTransformation<?, ?, ?>) {
    transformedIds = transformTwoInputTransform(
      (TwoInputTransformation<?, ?, ?>) transform);
  } else if (transform instanceof AbstractMultipleInputTransformation<?>) {
    transformedIds = transformMultipleInputTransform(
      (AbstractMultipleInputTransformation<?>) transform);
  } else if (transform instanceof SourceTransformation) {
    transformedIds = transformSource(
      (SourceTransformation<?>) transform);
  } else if (transform instanceof LegacySourceTransformation<?>) {
    transformedIds = transformLegacySource(
      (LegacySourceTransformation<?>) transform);
  } else if (transform instanceof SinkTransformation<?>) {
    transformedIds = transformSink(
      (SinkTransformation<?>) transform);
  } else if (transform instanceof UnionTransformation<?>) {
    transformedIds = transformUnion(
      (UnionTransformation<?>) transform);
  } else if (transform instanceof SplitTransformation<?>) {
    transformedIds = transformSplit(
      (SplitTransformation<?>) transform);
  } else if (transform instanceof SelectTransformation<?>) {
    transformedIds = transformSelect(
      (SelectTransformation<?>) transform);
  } else if (transform instanceof FeedbackTransformation<?>) {
    transformedIds = transformFeedback(
      (FeedbackTransformation<?>) transform);
  } else if (transform instanceof CoFeedbackTransformation<?>) {
    transformedIds = transformCoFeedback(
      (CoFeedbackTransformation<?>) transform);
  } else if (transform instanceof PartitionTransformation<?>) {
    transformedIds = transformPartition(
      (PartitionTransformation<?>) transform);
  } else if (transform instanceof SideOutputTransformation<?>) {
    transformedIds = transformSideOutput(
      (SideOutputTransformation<?>) transform);
  } else {
    throw new IllegalStateException("Unknown transformation: " + transform);
  }

  // need this check because the iterate transformation adds itself before
  // transforming the feedback edges
  if (!alreadyTransformed.containsKey(transform)) {
    alreadyTransformed.put(transform, transformedIds);
  }

  if (transform.getBufferTimeout() >= 0) {
    streamGraph.setBufferTimeout(transform.getId(), transform.getBufferTimeout());
  } else {
    streamGraph.setBufferTimeout(transform.getId(), defaultBufferTimeout);
  }

  if (transform.getUid() != null) {
    streamGraph.setTransformationUID(transform.getId(), transform.getUid());
  }
  if (transform.getUserProvidedNodeHash() != null) {
    streamGraph.setTransformationUserHash(transform.getId(), transform.getUserProvidedNodeHash());
  }

  if (!streamGraph.getExecutionConfig().hasAutoGeneratedUIDsEnabled()) {
    if (transform instanceof PhysicalTransformation &&
        transform.getUserProvidedNodeHash() == null &&
        transform.getUid() == null) {
      throw new IllegalStateException("Auto generated UIDs have been disabled " + "but no UID or hash has been assigned to operator " + transform.getName());
    }
  }

  if (transform.getMinResources() != null && transform.getPreferredResources() != null) {
    streamGraph.setResources(transform.getId(), transform.getMinResources(), transform.getPreferredResources());
  }

  streamGraph.setManagedMemoryWeight(transform.getId(), transform.getManagedMemoryWeight());

  return transformedIds;
}
```

最终都会调用 `transformXXX` 来对具体的StreamTransformation进行转换。

在debug 时, 发现第一个调用的 transformXXX 方法是 `transformLegacySource(`
`LegacySourceTransformation<?>) ` 不是预想中的 `transformOneInputTransform(OneInputTransformation<?, ?>)`

```java
private <T> Collection<Integer> transformLegacySource(
  								LegacySourceTransformation<T> source) {
 
  String slotSharingGroup = determineSlotSharingGroup(
    													source.getSlotSharingGroup(), 
    													Collections.emptyList());

  streamGraph.addLegacySource(source.getId(),
                              slotSharingGroup,
                              source.getCoLocationGroupKey(),
                              source.getOperatorFactory(),
                              null,
                              source.getOutputType(),
                              "Source: " + source.getName());
  if (source.getOperatorFactory() instanceof InputFormatOperatorFactory) {
    streamGraph.setInputFormat(source.getId(),
                               ((InputFormatOperatorFactory<T>) source.getOperatorFactory()).getInputFormat());
  }
  int parallelism = source.getParallelism() != ExecutionConfig.PARALLELISM_DEFAULT ?
    source.getParallelism() : executionConfig.getParallelism();
  streamGraph.setParallelism(source.getId(), parallelism);
  streamGraph.setMaxParallelism(source.getId(), source.getMaxParallelism());
  return Collections.singleton(source.getId());
}
```



> 实例信息:
>
> LegacySourceTransformation{id=1, name='Custom File Source', outputType=GenericType<org.apache.flink.streaming.api.functions.source.TimestampedFileInputSplit>, parallelism=1}

源码中的解释:

>LegacySourceTransformation:
>
>这代表一个数据源。由于它没有任何输入，因此实际上并不会进行任何转换，而是任何拓扑的 root Transformation。@param <T> 此源产生的数据元素的类型

StreamGraph 类中有 3 个字段:

```java
// StreamGraph
private Map<Integer, StreamNode> streamNodes;
private Set<Integer> sources;
private Set<Integer> sinks;
```

